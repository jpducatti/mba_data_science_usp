{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"flekT6GFDN6m"},"source":["# <span style=\"color:blue\">MBA em Ciência de Dados</span>\n","# <span style=\"color:blue\">Análise de Dados com Base em Processamento Massivo em Paralelo</span>\n","\n","## <span style=\"color:blue\">Avaliação Final - Notebook </span>\n","\n","**Material Produzido por:**<br>\n",">**Profa. Dra. Cristina Dutra de Aguiar**<br>\n","\n","\n","**CEMEAI - ICMC/USP São Carlos**\n","\n","Este *notebook* deve conter as respostas para as consultas analíticas solicitadas nas Questões 8, 9 e 10. É possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql. As consultas devem ser especificadas na seção 6, na célula indicada para resposta.\n","\n","**IMPORTANTE**\n","\n","- **As respostas para as Questões 8, 9 e 10 somente serão consideradas se forem especificadas no *notebook*. Portanto, independentemente da alternativa estar certa ou errada, se a consulta analítica correspondente não for especificada, a alternativa será considerada errada.** \n","\n","- **Caso uma alternativa esteja certa, porém a especificação da consulta analítica correspondente estiver errada no *notebook*, a alternativa será considerada errada.**\n","\n","\n","O *notebook* contém a constelação de fatos da BI Solutions que deve ser utilizada para responder às questões e também todas as bibliotecas, bases de dados, inicializações, instalações, importações, geração de dataFrames, geração de visões temporárias e conversão dos tipos de dados necessárias para a realização da questão. Portanto, o *notebook* está preparado para ser executado usando Pandas, o método spark.sql() e os métodos do módulo pyspark.sql.\n","\n","O uso do *framework* Spark requer diversas configurações no ambiente de desenvolvimento para executar o *notebook*. Dado que tal complexidade foge do escopo de nossa disciplina, recomenda-se que o *notebook* seja executado na plataforma de desenvolvimento COLAB. O uso do COLAB  proporciona um ambiente de desenvolvimento pré-configurado e remove a complexidade de instalação e configuração de pacotes e *frameworks* que são utilizados na disciplina.\n","\n","**INSTRUÇÕES DE ENTREGA**\n","\n","**O que deve ser entregue:**\n","- **O notebook com as respostas no formato .ipynb**\n","- **O notebook com as respostas no formato .pdf**\n","\n","**Ambos arquivos devem ser nomeados usando o primeiro nome e o último sobrenome do aluno. Por exemplo: CristinaAguiar.ipynb e CristinaAguiar.pdf.**\n","\n","Boa avaliação!"]},{"cell_type":"markdown","metadata":{"id":"3o3dN_WLQcyD"},"source":["#1 Constelação de Fatos da BI Solutions\n","\n","A aplicação de *data warehousing* da BI Solutions utiliza como base uma constelação de fatos, conforme descrita a seguir.\n","\n","**Tabelas de dimensão**\n","\n","- data (dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno)\n","- funcionario (funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla)\n","- equipe (equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla)\n","- cargo (cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel)\n","- cliente (clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla)\n","\n","**Tabelas de fatos**\n","- pagamento (dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamentos)\n","- negociacao (dataPK, equipePK, clientePK, receita, quantidadeNegociacoes)\n"]},{"cell_type":"markdown","metadata":{"id":"BGeh8KdXwVCQ"},"source":["#2 Obtenção dos Dados da BI Solutions\n"]},{"cell_type":"markdown","metadata":{"id":"CCCNC64AzBG0"},"source":["## 2.1 Baixando o Módulo wget\n","\n","Para baixar os dados referentes ao esquema relacional da constelação de fatos da BI Solutions, é utilizado o módulo  **wget**. O comando a seguir realiza a instalação desse módulo. <br>"]},{"cell_type":"code","metadata":{"id":"3e0Eao1K0EYG"},"source":["#instalando o módulo wget\n","%%capture\n","!pip install -q wget\n","!mkdir data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j56pVJ2hZ2i5"},"source":["## 2.2 Obtenção dos Dados das Tabelas de Dimensão\n","\n","Os comandos a seguir baixam os dados que povoam as tabelas de dimensão. "]},{"cell_type":"code","metadata":{"id":"46QzTpLJwfkW","cellView":"both","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1668044852089,"user_tz":180,"elapsed":844,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"8667f583-a13f-4091-c28f-e594faf9289d"},"source":["#baixando os dados das tabelas de dimensão\n","import wget\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv\"\n","wget.download(url, \"data/data.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv\"\n","wget.download(url, \"data/funcionario.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv\"\n","wget.download(url, \"data/equipe.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv\"\n","wget.download(url, \"data/cargo.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv\"\n","wget.download(url, \"data/cliente.csv\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data/cliente (1).csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"0o-dC7feszRc"},"source":["## 2.3 Obtenção dos Dados Tabelas de Fatos\n","\n","Os comandos a seguir baixam os dados que povoam as tabelas de fatos. "]},{"cell_type":"code","metadata":{"id":"XWM-CUFgBl_8","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1668044852671,"user_tz":180,"elapsed":584,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"574f02c5-52b0-453d-b49e-ec0b9201a71f"},"source":["#baixando os dados das tabelas de fatos\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv\"\n","wget.download(url, \"data/pagamento.csv\")\n","\n","url = \"https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv\"\n","wget.download(url, \"data/negociacao.csv\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'data/negociacao (1).csv'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":35}]},{"cell_type":"markdown","metadata":{"id":"sO16-7-jOioq"},"source":["# 3 Apache Spark Cluster"]},{"cell_type":"markdown","metadata":{"id":"YVEgY9qKflBV"},"source":["## 3.1 Instalação\n","\n","Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."]},{"cell_type":"markdown","metadata":{"id":"KaM-OnIjgLS2"},"source":["Para que o cluster possa ser criado, primeiramente é instalado o Java Runtime Environment (JRE) versão 8. "]},{"cell_type":"code","metadata":{"id":"NXls3bfoglKW"},"source":["#instalando Java Runtime Environment (JRE) versão 8\n","%%capture\n","!apt-get remove openjdk*\n","!apt-get update --fix-missing\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BQzZfDYhb4j"},"source":["Na sequência, é feito o *download* do Apache Spark versão 3.0.0."]},{"cell_type":"code","metadata":{"id":"8a_Yv59zg3gm"},"source":["#baixando Apache Spark versão 3.0.0\n","%%capture\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RETWX6wqhkLf"},"source":["Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."]},{"cell_type":"code","metadata":{"id":"iZpR7NwOh2EB"},"source":["import os\n","#configurando a variável de ambiente JAVA_HOME\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#configurando a variável de ambiente SPARK_HOME\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ql0z7Ro1iHQb"},"source":["Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n","\n","> **Pacote findspark:** Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark. \n","\n","> **Pacote pyspark:** PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o *framework* Apache Spark encontra-se desenvolvido na linguagem de programação Scala. "]},{"cell_type":"code","metadata":{"id":"5oSYOwKljPf5"},"source":["%%capture\n","#instalando o pacote findspark\n","!pip install -q findspark==1.4.2\n","#instalando o pacote pyspark\n","!pip install -q pyspark==3.0.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eAaLyjPzmIwZ"},"source":["## 3.2 Conexão\n","\n","PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n","\n","Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade.\n"]},{"cell_type":"code","metadata":{"id":"-zm1pBTEmjp4"},"source":["#importando o módulo findspark\n","import findspark\n","#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZDqfefF7YUab"},"source":["Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível iniciar o uso do Spark na aplicação de `data warehousing`. Para tanto, é necessário importar o comando `SparkSession` do módulo `pyspark.sql`. São utilizados os seguintes conceitos: <br>\n","\n","- `SparkSession`: permite a criação de `DataFrames`. Como resultado, as tabelas relacionais podem ser manipuladas por meio de `DataFrames` e é possível realizar consultas OLAP por meio de comandos SQL. <br>\n","- `builder`: cria uma instância de SparkSession. <br>\n","- `appName`: define um nome para a aplicação, o qual pode ser visto na interface de usuário web do Spark. <br> \n","- `master`: define onde está o nó mestre do *cluster*. Como a aplicação é executada localmente e não em um *cluster*, indica-se isso pela *string* `local` seguida do parâmetro `[*]`. Ou seja, define-se que apenas núcleos locais são utilizados. \n","- `getOrCreate`: cria uma SparkSession. Caso ela já exista, retorna a instância existente. \n","\n","\n","**Observação**: A lista completa de todos os parâmetros que podem ser utilizados na inicialização do *cluster* pode ser encontrada neste [link](https://spark.apache.org/docs/latest/spark-standalone.html#cluster-launch-scripts)."]},{"cell_type":"code","metadata":{"id":"9TxljJ_cwBCy"},"source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"pyspark-notebook\").master(\"local[*]\").getOrCreate()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2IpYfoG_kx_8"},"source":["# 4 Geração dos DataFrames em Pandas da BI Solutions\n","\n","Nesta seção são gerados os DataFrames em Pandas. Atenção aos nomes desses DataFrames. \n"]},{"cell_type":"code","metadata":{"id":"9arYf_PHlJCR"},"source":["import pandas as pd\n","pd.set_option('display.float_format', lambda x: '%.2f' % x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qw4NfyDZ--6z"},"source":["cargoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cargo.csv')\n","clientePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/cliente.csv')\n","dataPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/data.csv')\n","equipePandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/equipe.csv')\n","funcionarioPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/funcionario.csv')\n","negociacaoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/negociacao.csv')\n","pagamentoPandas = pd.read_csv('https://raw.githubusercontent.com/GuiMuzziUSP/Data_Mart_BI_Solutions/main/pagamento.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5qL9SiR_pQE2"},"source":["# 5 Geração dos DataFrames em Spark da BI Solutions\n","\n","Nesta seção são gerados dos DataFrames em Spark. Atenção aos nomes desses DataFrames. \n"]},{"cell_type":"markdown","metadata":{"id":"DRVoz-SGt87W"},"source":["## 5.1 Criação dos DataFrames"]},{"cell_type":"code","metadata":{"cellView":"both","id":"FNR-3dV6oYk4"},"source":["#criando os DataFrames em Spark \n","cargo = spark.read.csv(path=\"data/cargo.csv\", header=True, sep=\",\")\n","cliente = spark.read.csv(path=\"data/cliente.csv\", header=True, sep=\",\")\n","data = spark.read.csv(path=\"data/data.csv\", header=True, sep=\",\")\n","equipe = spark.read.csv(path=\"data/equipe.csv\", header=True, sep=\",\")\n","funcionario = spark.read.csv(path=\"data/funcionario.csv\", header=True, sep=\",\")\n","negociacao = spark.read.csv(path=\"data/negociacao.csv\", header=True, sep=\",\")\n","pagamento = spark.read.csv(path=\"data/pagamento.csv\", header=True, sep=\",\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mrch9vLgjl_H"},"source":["## 5.2 Atualização dos Tipos de Dados "]},{"cell_type":"markdown","metadata":{"id":"9A_ot2pOjsWB"},"source":["Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado inteiro. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."]},{"cell_type":"code","metadata":{"id":"jmCV6Mur__z6"},"source":["# identificando quais colunas de quais DataFrames devem ser do tipo de dado inteiro\n","colunas_cargo = [\"cargoPK\"]\n","colunas_cliente = [\"clientePK\"]\n","colunas_data = [\"dataPK\", \"dataDia\", \"dataMes\", \"dataBimestre\", \"dataTrimestre\", \"dataSemestre\", \"dataAno\"]\n","colunas_equipe = [\"equipePK\"]\n","colunas_funcionario = [\"funcPK\", \"funcDiaNascimento\", \"funcMesNascimento\", \"funcAnoNascimento\"]\n","colunas_negociacao = [\"equipePK\", \"clientePK\", \"dataPK\", \"quantidadeNegociacoes\"]\n","colunas_pagamento = [\"funcPK\", \"equipePK\", \"dataPK\", \"cargoPK\", \"quantidadeLancamentos\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yPNnDJcG9R5H"},"source":["# importando o tipo de dado desejado\n","from pyspark.sql.types import IntegerType\n","\n","\n","# atualizando o tipo de dado das colunas especificadas \n","# substituindo as colunas já existentes \n","\n","for coluna in colunas_cargo:\n","  cargo = cargo.withColumn(coluna, cargo[coluna].cast(IntegerType()))\n","\n","for coluna in colunas_cliente:\n","  cliente = cliente.withColumn(coluna, cliente[coluna].cast(IntegerType()))\n","\n","for coluna in colunas_data:\n","  data = data.withColumn(coluna, data[coluna].cast(IntegerType()))\n","\n","for coluna in colunas_equipe:\n","  equipe = equipe.withColumn(coluna, equipe[coluna].cast(IntegerType()))\n","\n","for coluna in colunas_funcionario:\n","  funcionario = funcionario.withColumn(coluna, funcionario[coluna].cast(IntegerType()))\n","\n","for coluna in colunas_negociacao:\n","  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(IntegerType()))\n","\n","for coluna in colunas_pagamento:\n","  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(IntegerType()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J0dX_7U_AzIY"},"source":["Nos comandos a seguir, primeiro são identificados quais colunas de quais `DataFrames` devem ser do tipo de dado número de ponto flutuante. Na sequência, ocorre a conversão. Por fim, são exibidos os esquemas dos `DataFrames`, possibilitando visualizar a mudança de tipo de dados das colunas especificadas."]},{"cell_type":"code","metadata":{"id":"RBcQ7Ep7AWqN"},"source":["# identificando quais colunas de quais DataFrames devem ser do tipo de dado número de ponto flutuante\n","colunas_negociacao = [\"receita\"]\n","colunas_pagamento = [\"salario\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rcfvkIK1BRSp"},"source":["# importando o tipo de dado desejado\n","from pyspark.sql.types import FloatType\n","\n","\n","# atualizando o tipo de dado das colunas especificadas \n","# substituindo as colunas já existentes \n","\n","for coluna in colunas_negociacao:\n","  negociacao = negociacao.withColumn(coluna, negociacao[coluna].cast(FloatType()))\n","\n","for coluna in colunas_pagamento:\n","  pagamento = pagamento.withColumn(coluna, pagamento[coluna].cast(FloatType()))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"91nwfsV3_rKs"},"source":["# importando funções adicionais \n","from pyspark.sql.functions import round, desc"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9wN5iOGKwnHG"},"source":["## 5.3 Criação de Visões Temporárias"]},{"cell_type":"code","metadata":{"id":"xJsqRI3TwsjS"},"source":["#criando as visões temporárias \n","cargo.createOrReplaceTempView(\"cargo\")\n","cliente.createOrReplaceTempView(\"cliente\")\n","data.createOrReplaceTempView(\"data\")\n","equipe.createOrReplaceTempView(\"equipe\")\n","funcionario.createOrReplaceTempView(\"funcionario\")\n","negociacao.createOrReplaceTempView(\"negociacao\")\n","pagamento.createOrReplaceTempView(\"pagamento\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6 Respostas"],"metadata":{"id":"2X_PNhxFCKh6"}},{"cell_type":"markdown","source":["Lembre-se que é possível especificar as consultas analíticas usando Pandas, o método spark.sql() ou os métodos do módulo pyspark.sql."],"metadata":{"id":"vqsvtdy4CdVw"}},{"cell_type":"markdown","metadata":{"id":"bcSouuCk6N0-"},"source":["## Especificação da Consulta Analítica da Questão 8"]},{"cell_type":"code","metadata":{"id":"3kKTtt-V6TKI"},"source":["# Resposta da Questão 8\n","# Não se esqueça de comentar detalhadamente a sua solução\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fpFnRTk369Q5"},"source":["## Especificação da Consulta Analítica da Questão 9"]},{"cell_type":"code","metadata":{"id":"qTbsaLSirPyD"},"source":["# Resposta da Questão 9\n","# Não se esqueça de comentar detalhadamente a sua solução\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zCpy46lL8Xdn"},"source":["## Especificação das Consultas Analíticas da Questão 10"]},{"cell_type":"code","metadata":{"id":"Sq3uXrzT8jO-"},"source":["# Resposta da Questão 10 - Primeira Consulta\n","# Não se esqueça de comentar detalhadamente a sua solução\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Resposta da Questão 10 - Segunda Consulta\n","# Não se esqueça de comentar detalhadamente a sua solução\n"," "],"metadata":{"id":"X5RIvWkhIMP3"},"execution_count":null,"outputs":[]}]}