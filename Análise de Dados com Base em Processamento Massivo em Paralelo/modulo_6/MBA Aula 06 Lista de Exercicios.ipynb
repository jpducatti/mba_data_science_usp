{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["# MBA em Ciência de Dados - Análise de Dados com Base em Processamento Massivo em Paralelo ##\n","\n","### Aula 06: Processamento Paralelo e Distribuído\n","\n","## Lista de Exercícios ##"],"metadata":{"id":"WLISLDOkYs9q"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"qpVtHzdJYvZ5"}},{"cell_type":"markdown","metadata":{"id":"2QbaUWeQxHYR"},"source":["**Material Produzido por:**<br>\n",">**Profa. Dra. Cristina Dutra de Aguiar**<br>\n","\n","**CEMEAI - ICMC/USP São Carlos**\n","\n","Esta lista possui 11 exercícios, sendo possível navegar pelos mesmos utilizando o sumário na esquerda. Os primeiros exercícios contam com dicas para auxiliar na sua resolução. Leiam-os com atenção e desenvolvam as respostas nos blocos indicados com \n","```\n","# Resposta do exercício\n","```\n","\n","Por completude, o *notebook* possui todas as descrições apresentadas na parte prática da Aula 06. **Recomenda-se fortemente** que esta lista seja respondida antes de se consultar o material com as respostas.\n"]},{"cell_type":"markdown","metadata":{"id":"AWKkx5f8xTMM"},"source":["# 1 Introdução\n","\n","A aplicação de *data warehousing* da BI Solutions utiliza como base uma contelação de fatos que une dois esquemas estrela, conforme descrito a seguir.\n","\n","**Tabelas de dimensão**\n","\n","- data (`dataPK, dataCompleta, dataDia, dataMes, dataBimestre, dataTrimestre, dataSemestre, dataAno`)\n","- funcionario (`funcPK, funcMatricula, funcNome, funcSexo, funcDataNascimento, funcDiaNascimento, funcMesNascimento, funcAnoNascimento, funcCidade, funcEstadoNome, funcEstadoSigla, funcRegiaoNome, funcRegiaoSigla, funcPaisNome, funcPaisSigla`)\n","- equipe (`equipePK, equipeNome, filialNome, filialCidade, filialEstadoNome, filialEstadoSigla, filialRegiaoNome, filialRegiaoSigla, filialPaisNome, filialPaisSigla`)\n","- cargo (`cargoPK, cargoNome, cargoRegimeTrabalho, cargoEscolaridadeMinima, cargoNivel`)\n","- cliente (`clientePK, clienteNomeFantasia, clienteSetor, clienteCidade, clienteEstadoNome, clienteEstadoSigla, clienteRegiaoNome, clienteRegiaoSigla, clientePaisNome, clientePaisSigla`)\n","\n","**Tabelas de fatos**\n","- pagamento (`dataPK, funcPK, equipePK, cargoPK, salario, quantidadeLancamento`)\n","- negociacao (`dataPK, equipePK, clientePK, receita, quantidadeNegociacao`)"]},{"cell_type":"markdown","metadata":{"id":"vYmvShw4vfP5"},"source":["Primeiramente, são definidos `paths`, sendo que cada `path` se refere a uma tabela de fatos ou uma tabela de dimensão. "]},{"cell_type":"code","metadata":{"id":"G1kwE479vk4N"},"source":["# Tabelas de dimensão\n","pathData = 'dados/data.csv'\n","pathFuncionario = 'dados/funcionario.csv'\n","pathEquipe = 'dados/equipe.csv'\n","pathCargo = 'dados/cargo.csv'\n","pathCliente = 'dados/cliente.csv'\n","\n","# Tabelas de fato\n","pathPagamento = 'dados/pagamento.csv'\n","pathNegociacao = 'dados/negociacao.csv'"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hiIhXOKe11No"},"source":["Na sequência,  todos os arquivos referentes às tabelas de fatos e às tabelas de dimensão são baixados, sendo armazenados na pasta `dados`."]},{"cell_type":"code","metadata":{"id":"kHsUKNlvvaBT"},"source":["%%capture\n","!git clone https://github.com/GuiMuzziUSP/Data_Mart_BI_Solutions.git dados"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlYaf6zo0W_B"},"source":["# 2 Apache Spark Cluster: instalação e configuração"]},{"cell_type":"markdown","metadata":{"id":"5Zl1gR_qy1PD"},"source":["**2.1 Instalação**\n","\n","Neste *notebook* é criado um *cluster* Spark composto apenas por um **nó mestre**. Ou seja, o *cluster* não possui um ou mais **nós de trabalho** e o **gerenciador de cluster**. Nessa configuração, as tarefas (*tasks*) são realizadas no próprio *driver* localizado no **nó mestre**."]},{"cell_type":"code","metadata":{"id":"vjO0nAeqyRna"},"source":["#instalando Java Runtime Environment (JRE) versão 8\n","%%capture\n","!apt-get remove openjdk*\n","!apt-get update --fix-missing\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FDL6HLRhzHzL"},"source":["Na sequência, é feito o *download* do Apache Spark versão 3.0.0."]},{"cell_type":"code","metadata":{"id":"5ItvOmwstGOO"},"source":["#baixando Apache Spark versão 3.0.0\n","%%capture\n","!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop2.7.tgz\n","!tar xf spark-3.0.0-bin-hadoop2.7.tgz && rm spark-3.0.0-bin-hadoop2.7.tgz"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcPvn_vgtJ6l"},"source":["Na sequência, são configuradas as variáveis de ambiente JAVA_HOME e SPARK_HOME. Isto permite que tanto o Java quanto o Spark possam ser encontrados."]},{"cell_type":"code","metadata":{"id":"HAXvU63xtRck"},"source":["import os\n","#configurando a variável de ambiente JAVA_HOME\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","#configurando a variável de ambiente SPARK_HOME\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop2.7\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n8hAOJtBtY7M"},"source":["Por fim, são instalados dois pacotes da linguagem de programação Python, cujas funcionalidades são descritas a seguir.\n","\n","Pacote findspark: Usado para ler a variável de ambiente SPARK_HOME e armazenar seu valor na variável dinâmica de ambiente PYTHONPATH. Como resultado, Python pode encontrar a instalação do Spark.\n","\n","Pacote pyspark: PySpark é a API do Python para Spark. Ela possibilita o uso de Python, considerando que o framework Apache Spark encontra-se desenvolvido na linguagem de programação Scala."]},{"cell_type":"code","metadata":{"id":"X-0U9STdtZ4Y"},"source":["%%capture\n","#instalando o pacote findspark\n","!pip install -q findspark==1.4.2\n","#instalando o pacote pyspark\n","!pip install -q pyspark==3.0.0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xrBLEKI0OWx"},"source":["**2.2 Conexão**\n","\n","PySpark não é adicionado ao *sys.path* por padrão. Isso significa que não é possível importá-lo, pois o interpretador da linguagem Python não sabe onde encontrá-lo. \n","\n","Para resolver esse aspecto, é necessário instalar o módulo `findspark`. Esse módulo mostra onde PySpark está localizado. Os comandos a seguir têm essa finalidade."]},{"cell_type":"code","metadata":{"id":"0RAqybyZYKdn"},"source":["#importando o módulo findspark\n","import findspark\n","#carregando a variávels SPARK_HOME na variável dinâmica PYTHONPATH\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bJjl5aEvtnur"},"source":["Depois de configurados os pacotes e módulos e inicializadas as variáveis de ambiente, é possível criar o objeto SparkContext. No comando de criação a seguir, é definido que é utilizado o próprio sistema operacional deste notebook como nó mestre por meio do parâmetro local do método setMaster. O complemento do parametro [*] indica que são alocados todos os núcleos de processamento disponíveis para o objeto driver criado."]},{"cell_type":"code","metadata":{"id":"KB7ZlgxIVss3"},"source":["from pyspark import SparkConf, SparkContext\n","\n","conf = SparkConf().setMaster(\"local[*]\")\n","spark = SparkContext(conf=conf)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c7SWE09n1sug"},"source":["# 3 Carregamento dos Dados da Aplicação da BI Solutions"]},{"cell_type":"markdown","metadata":{"id":"G8D5I5Fb5YrL"},"source":["**3.1 Carregamento da tabela de dimensão** *data*"]},{"cell_type":"markdown","metadata":{"id":"QPli-rPKjCMh"},"source":["O comando a seguir utiliza o método `textFile()` para armazenar no RDD chamado `data` os registros do arquivo de texto `\"data.csv\"`, os quais possuem os dados da tabela de dimensão `data`."]},{"cell_type":"code","metadata":{"id":"iWOM5WsQf2pa"},"source":["data_rdd = spark.textFile(pathData)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2076gqycn_yS"},"source":["Os comandos a seguir realizam alterações no RDD `data` de forma que seus elementos representem linhas (ou tuplas) da tabela."]},{"cell_type":"code","metadata":{"id":"FGN8-kownl0e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666900999771,"user_tz":180,"elapsed":1699,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"da94bb21-f050-46ca-ebc8-643c4a963a89"},"source":["#imprimindo as 3 primeiras linhas de \"data\" e verificando que a primeira linha contém metadados (ou seja, o esquema referente aos dados)\n","data_rdd.take(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['dataPK,dataCompleta,dataDia,dataMes,dataBimestre,dataTrimestre,dataSemestre,dataAno',\n"," '1,1/1/2016,1,1,1,1,1,2016',\n"," '2,2/1/2016,2,1,1,1,1,2016']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"mjJZjQySf2ho"},"source":["#removendo a primeira linha de \"data\", desde que ela se refere a metadados\n","#capturando o cabeçalho\n","firstRow = data_rdd.first()\n","#removendo o cabeçalho\n","data_rdd = data_rdd.filter(lambda line: line != firstRow)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cAFiGv_qycSu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666900999771,"user_tz":180,"elapsed":8,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"0b52ad73-0699-4e9e-9271-faa0ff20c66e"},"source":["#imprimindo as 3 primeiras linhas de \"data\" e verificando que elas contêm apenas dados\n","data_rdd.take(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1,1/1/2016,1,1,1,1,1,2016',\n"," '2,2/1/2016,2,1,1,1,1,2016',\n"," '3,3/1/2016,3,1,1,1,1,2016']"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"qvUk1tNTnCbO","colab":{"base_uri":"https://localhost:8080/","height":159},"executionInfo":{"status":"ok","timestamp":1666900999772,"user_tz":180,"elapsed":8,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"850d4923-b6f6-44b4-e4f5-853204b1228d"},"source":["#imprimindo o cabeçalho de \"data\" e verificando os metadados \n","data_header = firstRow[:].split(\",\")\n","display(data_header)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["['dataPK',\n"," 'dataCompleta',\n"," 'dataDia',\n"," 'dataMes',\n"," 'dataBimestre',\n"," 'dataTrimestre',\n"," 'dataSemestre',\n"," 'dataAno']"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Rm57ZD8bhE6G"},"source":["Desde que o arquivo lido encontra-se no formato `.csv`, utiliza-se o método `()` para transformar os elementos do RDD `data` em uma lista de valores divididos por `\",\"`."]},{"cell_type":"code","metadata":{"id":"yMt3gxSLf2d7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666900999772,"user_tz":180,"elapsed":7,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"975c0dba-a53f-4c2d-891e-8f728e1865b8"},"source":["#mapeando os valores do RDD \"data\" utilizando o separador vírgula\n","data_rdd = data_rdd.map(lambda line: tuple(line.split(\",\")))\n","#imprimindo as 3 primeiras linhas de \"data\"\n","data_rdd.take(3)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('1', '1/1/2016', '1', '1', '1', '1', '1', '2016'),\n"," ('2', '2/1/2016', '2', '1', '1', '1', '1', '2016'),\n"," ('3', '3/1/2016', '3', '1', '1', '1', '1', '2016')]"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"bCFWrvu8hSNx"},"source":["# **Exercícios**"]},{"cell_type":"markdown","metadata":{"id":"NzmenlB1etNP"},"source":["#### **Exercício 1**\n","\n","Dê continuidade ao exemplo anterior e realize o carregamento dos demais arquivos referentes à constelação de fatos da BI solution, a saber: (i) tabelas de dimensão `funcionario`, `equipe`, `cargo` e `cliente`; e (ii) tabelas de fato `pagamento` e `negociação`."]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"aTRKDJN2ats3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Dica 1 para o exercício**: Crie uma função para executar este procedimento repetidas vezes. Utilize o esqueleto a seguir como base.\n","```python\n","def processaRdd(spark, path):\n","  ...\n","  return header, rddCsv\n","```\n","**Dica 2**: Se tiver dificuldades em criar a função, replique os comandos descritos para o carregamento da tabela de dimensão data para todas as tabelas restantes."],"metadata":{"id":"It9_eETEFQxh"}},{"cell_type":"markdown","metadata":{"id":"Kugu7q5SY_qu"},"source":["### **Exercício 2**\n","\n","Selecione as seguintes colunas do RDD `cliente`: primeira, segunda, terceira e sexta. Exiba as 5 primeiras linhas resultantes. Consulte `cliente_header` caso necessário. "]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"LrRbCnJPa0Be"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jb07Xgblptuf"},"source":["**Dica para o exercício:** método ***map()***\n","\n","O método `map()` pode ser utilizado para selecionar colunas. Veja o exemplo abaixo:"]},{"cell_type":"code","metadata":{"id":"Sq268iE9450V","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666902093030,"user_tz":180,"elapsed":258,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"1df42c99-06e4-4565-afd3-6a704c856e13"},"source":["#selecionando as seguintes colunas do RDD \"funcionario\":  segunda, terceira, décima\n","funcionario_rdd \\\n","  .map(lambda x: (x[1], x[2], x[9])) \\\n","  .take(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('M-1', 'ALINE ALMEIDA', 'SAO PAULO'),\n"," ('M-2', 'ARAO ALVES', 'SAO PAULO'),\n"," ('M-3', 'ARON ANDRADE', 'SAO PAULO'),\n"," ('M-4', 'ADA BARBOSA', 'SAO PAULO'),\n"," ('M-5', 'ABADE BATISTA', 'SAO PAULO')]"]},"metadata":{},"execution_count":44}]},{"cell_type":"markdown","metadata":{"id":"6W4su6k1aGkZ"},"source":["### **Exercício 3**\n","\n","Recupere os clientes que moram no estado de Minas Gerais."]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"u7X0ojI6a4rb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5Ng3iGSNsnNH"},"source":["**Dica para o exercício**: Método ***filter()***\n","\n","O método `filter()` pode ser utilizado para filtrar valores de acordo com  critérios de seleção. No exemplo a seguir, o método `filter()` é utilizado para filtrar os funcionários que **não** são do estado de `'SAO PAULO'`."]},{"cell_type":"code","source":["cliente_rdd.filter(lambda x: x[5] != \"SP\").take(7)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T9yGqmXzH4zp","executionInfo":{"status":"ok","timestamp":1666902433403,"user_tz":180,"elapsed":269,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"c5009283-2cea-4e52-cca0-635161824232"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('7',\n","  'VIA PASTA',\n","  'BEBIDAS E ALIMENTOS',\n","  'RIO DE JANEIRO',\n","  'RIO DE JANEIRO',\n","  'RJ',\n","  'SUDESTE',\n","  'SE',\n","  'BRASIL',\n","  'BR'),\n"," ('8',\n","  'VIA FRIENDS',\n","  'BEBIDAS E ALIMENTOS',\n","  'RIO DE JANEIRO',\n","  'RIO DE JANEIRO',\n","  'RJ',\n","  'SUDESTE',\n","  'SE',\n","  'BRASIL',\n","  'BR'),\n"," ('9',\n","  'VIA HAPPY HOUR',\n","  'BEBIDAS E ALIMENTOS',\n","  'RIO DE JANEIRO',\n","  'RIO DE JANEIRO',\n","  'RJ',\n","  'SUDESTE',\n","  'SE',\n","  'BRASIL',\n","  'BR'),\n"," ('10',\n","  'VIA LIFE',\n","  'SAUDE',\n","  'BELO HORIZONTE',\n","  'MINAS GERAIS',\n","  'MG',\n","  'SUDESTE',\n","  'SE',\n","  'BRASIL',\n","  'BR'),\n"," ('11',\n","  'VIA MED',\n","  'SAUDE',\n","  'UBERLANDIA',\n","  'MINAS GERAIS',\n","  'MG',\n","  'SUDESTE',\n","  'SE',\n","  'BRASIL',\n","  'BR'),\n"," ('12',\n","  'VIA PREVENT',\n","  'SAUDE',\n","  'SALVADOR',\n","  'CEARA',\n","  'CE',\n","  'NORDESTE',\n","  'NE',\n","  'BRASIL',\n","  'BR'),\n"," ('13',\n","  'VIA SENIOR',\n","  'SAUDE',\n","  'RECIFE',\n","  'PERNAMBUCO',\n","  'PE',\n","  'NORDESTE',\n","  'NE',\n","  'BRASIL',\n","  'BR')]"]},"metadata":{},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"2C8HjkEpPHyw"},"source":["### **Exercício 4**\n","\n","Realize a junção da tabela cliente com a tabela negociacao, considerando a integridade referencial definida em termos de clientePK (ou seja, `cliente.clientePK = negociacao.clientePK`). "]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"VX2TbhgSa8_V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"e4IYWDCguVhF"},"source":["**Dica para exercício:** Método ***join()***\n","\n","O método `join()` pode ser utilizado para juntar duas tabelas de acordo com a integridade referencial, ou seja, de acordo com a chave primária presente em uma primeira tabela e a chave secundária presente em uma segunda tabela.\n","\n","No exemplo a seguir, o método `join()` é utilizado para juntar dados da tabela de dimensão funcionario com dados da tabela de dimensão pagamento, utilizando a junção estrela definida em termos de `funcionario.funcPK = pagamento.funcPK`"]},{"cell_type":"code","metadata":{"id":"q4ub2lviuHat","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1666901002307,"user_tz":180,"elapsed":6,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"914578df-eb00-4640-ef53-6415158a693a"},"source":["#listando os metadados de funcionário\n","display(funcionario_header)\n","\n","#criando um RDD temporário para funcionário, contendo apenas algumas colunas \n","funcionario_temp = funcionario_rdd \\\n","  .map(lambda x: (x[0], x[2], x[9]))\n","\n","#listando os 5 primeiros elementos de \"funcionario_temp\"\n","funcionario_temp \\\n","  .take(5)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["['funcPK',\n"," 'funcMatricula',\n"," 'funcNome',\n"," 'funcSexo',\n"," 'funcDataNascimento',\n"," 'funcDiaNascimento',\n"," 'funcMesNascimento',\n"," 'funcAnoNascimento',\n"," 'funcCidade',\n"," 'funcEstadoNome',\n"," 'funcEstadoSigla',\n"," 'funcRegiaoNome',\n"," 'funcRegiaoSigla',\n"," 'funcPaisNome',\n"," 'funcPaisSigla']"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[('1', 'ALINE ALMEIDA', 'SAO PAULO'),\n"," ('2', 'ARAO ALVES', 'SAO PAULO'),\n"," ('3', 'ARON ANDRADE', 'SAO PAULO'),\n"," ('4', 'ADA BARBOSA', 'SAO PAULO'),\n"," ('5', 'ABADE BATISTA', 'SAO PAULO')]"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"qTdpAtxIvJnv","colab":{"base_uri":"https://localhost:8080/","height":124},"executionInfo":{"status":"ok","timestamp":1666901002307,"user_tz":180,"elapsed":5,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"69443593-43a2-40c0-ab96-33dd7d0c0d50"},"source":["#listando os metadados de pagamento\n","display(pagamento_header)\n","\n","#criando um RDD temporário para pagamento, contendo apenas as colunas referentes à funcPK e às medidas numéricas\n","pagamento_temp = pagamento_rdd\\\n","  .map(lambda x: (x[0], x[4], x[1]))\n","\n","#listando os 5 primeiros elementos de \"pagamento_temp\" para o funcionário com funcPK = 5\n","def filterPagamentoFuncionario(func):\n","  return True if func[0] == '4' else False\n","pagamento_temp.filter(filterPagamentoFuncionario).take(5)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["['funcPK', 'equipePK', 'dataPK', 'cargoPK', 'salario', 'quantidadeLancamentos']"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[('4', '10498.14', '5'),\n"," ('4', '10498.14', '5'),\n"," ('4', '10498.14', '5'),\n"," ('4', '10498.14', '5'),\n"," ('4', '10498.14', '5')]"]},"metadata":{},"execution_count":26}]},{"cell_type":"code","metadata":{"id":"kwIve0rguezx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666901002717,"user_tz":180,"elapsed":414,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"45a5fa70-d7eb-44e1-f9c5-9858c14dd2f4"},"source":["#realizando a junção de funcionario_temp com pagamento_temp, para o funcionário com funcPK igual a 5\n","#note que a juncao é feita considerando a igualdade na primeira coluna\n","#note também que somente os valores da segunda coluna de funcionario e de pagamento são retornados \n","funcionario_temp \\\n","  .join(pagamento_temp) \\\n","  .take(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('4', ('ADA BARBOSA', '10498.14')),\n"," ('4', ('ADA BARBOSA', '10498.14')),\n"," ('4', ('ADA BARBOSA', '10498.14')),\n"," ('4', ('ADA BARBOSA', '10498.14')),\n"," ('4', ('ADA BARBOSA', '10498.14'))]"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"zOm-dvwzU73U"},"source":["### **Exercício 5** \n","\n","Liste a quantidade de clientes por região. Ordene o resultado pelo nome da região em ordem crescente."]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"kIMHAKiPbA9I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ul9bgXCoAxFH"},"source":["**Dica para o exercício**: Método ***reduceByKey()***\n","\n","O método `reduceByKey()` pode ser utilizado para calcular valores agregados para cada valor de chave presente em uma coluna. \n","\n","No exemplo a seguir, é listada a quantidade de funcionários por estado.\n","\n","Para responder a essa consulta, é necessário utilizar os dados de `funcionario_rdd`, usando a coluna `funcEstadoNome` e contando quantas vezes o mesmo nome de estado aparece. São executados os seguintes passos:\n","\n","- Utilização do método `map()` para selecionar a coluna desejada `funcEstadoNome`, que é a décima coluna de `funcionario_rdd`, e para criar pares chave-valor da seguinte forma: a chave corresponde à coluna `funcEstadoNome` e o valor corresponde a 1.\n","- Utilização do método `reduceByKey()` para calcular, para cada chave a quantidade de vezes que ela aparece.\n","- Utilização do método `collect()` para exibir os pares chave-valor obtidos."]},{"cell_type":"code","metadata":{"id":"6QS8GB9JRDQg","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"ok","timestamp":1666902738545,"user_tz":180,"elapsed":275,"user":{"displayName":"Cristina Aguiar","userId":"02805498492766545440"}},"outputId":"9d53f8a5-8d33-46dd-98c3-5c8da9c7f93c"},"source":["# Verificando quais colunas estão presentes no RDD funcionario\n","display(funcionario_header)\n","\n","# Executando os passos necessários para responder à consulta\n","funcionario_rdd \\\n","  .map(lambda x: (x[9], 1)) \\\n","  .reduceByKey(lambda x, y: x + y) \\\n","  .collect()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["['funcPK',\n"," 'funcMatricula',\n"," 'funcNome',\n"," 'funcSexo',\n"," 'funcDataNascimento',\n"," 'funcDiaNascimento',\n"," 'funcMesNascimento',\n"," 'funcAnoNascimento',\n"," 'funcCidade',\n"," 'funcEstadoNome',\n"," 'funcEstadoSigla',\n"," 'funcRegiaoNome',\n"," 'funcRegiaoSigla',\n"," 'funcPaisNome',\n"," 'funcPaisSigla']"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["[('MINAS GERAIS', 28),\n"," ('PARANA', 28),\n"," ('PERNAMBUCO', 21),\n"," ('SAO PAULO', 95),\n"," ('RIO DE JANEIRO', 28)]"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","metadata":{"id":"QFmWox4teixh"},"source":["### **Exercício 6** \n","\n","Qual foi o maior e o menor salário pago em 2019?\n"]},{"cell_type":"markdown","source":["\n","**Dica para o exercício**: Para responder a essa consulta, é necessário utilizar os dados de pagamento_rdd, usando a coluna salario e aplicando o método min() e max(), considerando a seguinte sequência de passos.\n","\n","- Utilização do método `map()` para transformar o tipo de dados da coluna salario em um número de ponto flutuante (ou seja, float)\n","\n","- Utilização do método `min()` para descobrir o menor salário.\n","\n","- Utilização do método `max()` para descobrir o maior salário."],"metadata":{"id":"4EoEZu2GXpmQ"}},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"zRzTxJF4bGKk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y6wTUtmqrvVg"},"source":["### **Exercício 7** \n","\n","Qual a idade média dos funcionários? Para fazer este exercício, não precisa considerar a idade exata, ou seja, não é necessário considerar o dia no qual o funcionário nasceu. Considere apenas o ano de nascimento do funcionário.  "]},{"cell_type":"markdown","metadata":{"id":"3djTT4Bqv3xZ"},"source":["**Exercício 7a** Resolva o exercício utilizando o método *mean()*."]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"SEJO76_zbJu5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"loLsid3QwfeW"},"source":["**Exercício 7b** Resolva o exercício **sem** usar o método *mean().*"]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"NfxY99p0bOj5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ED2laZHr4oPO"},"source":["### **Exercício 8**\n","\n","Liste a quantidade de vezes que cada palavra individual aparece nos nomes de cidades que os funcionários moram. Por exemplo, na cidade de SAO JOSE DO RIO PRETO, existem 5 palavras individuais. Ordene o resultado de forma que as palavras individuais que mais se repetem sejam mostradas primeiro. \n"]},{"cell_type":"code","source":["# Resposta do exercício"],"metadata":{"id":"mIPXKYwhbRAr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Exercício 9** \n","\n","Considere o código Python a seguir, o qual tem como objetivo contar quantas\n","vezes uma determinada palavra aparece em um arquivo texto com 4 linhas de log.\n","\n","```python\n","error_lines = [\"\"\"(2021-11-01 06:58:43) ERROR - Event with job id 1abc Failed,\n","                  (2021-11-01 06:58:47) ERROR - Event with job id 2abc Failed,\n","                  (2021-11-01 06:58:50) ERROR - Event with job id 3abc Failed,\n","                  (2021-11-01 06:59:43) INFO - Number of table lines is 102345.\"\"\"]\n","\n","output = spark.\\\n","parallelize(error_lines).\\\n","flatMap(lambda element: element.split(\" \")).\\\n","filter(lambda element: True if element == 'ERROR' else False).\\\n","count()\n","```\n","Explique qual é a função dos métodos *flatMap(), filter() e count(). Explique também qual o resultado da variável `output`.\n"],"metadata":{"id":"_ynnKV96RF7Z"}},{"cell_type":"markdown","source":["**Resposta do exercício:**"],"metadata":{"id":"Xwc4-FUsbUBQ"}},{"cell_type":"markdown","metadata":{"id":"Wo5CVe25TX8o"},"source":["### **Exercício 10**\n","\n","Considere a figura a seguir, a qual representa um cluster de computadores que segue\n","a arquitetura do sistema de arquivos distribuídos HDFS (*Hadoop Distributed File System*).\n","Ao lado de cada quadrado, existe uma numeração que representa o componente dentro\n","daquele quadrado. Responda às questões a seguir, utilizando como base essa numeração.\n","\n"]},{"cell_type":"markdown","source":["<br>\n","\n","<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kenjitakatuzi/MBA-ICMC-2022/main/images/CLUSTER.png\" width=\"500\" height=\"300\"></p>\n","<!-- <p align=\"center\">Figura 1 - Representação de um cluster de computadores que segue a arquitetura do HDFS. </p> -->\n","\n","<br>"],"metadata":{"id":"aILuDaa6YjUX"}},{"cell_type":"markdown","source":["**Resposta do exercício:**"],"metadata":{"id":"Ja2PWviBbb8F"}},{"cell_type":"markdown","source":["### **Exercício 11**\n","\n","Transformações são operações que transformam um RDD (Resilient and Distributed Datasets) em outro RDD. No Spark, as transformações não são executadas imediatamente sobre os dados do RDD. Elas são anexadas ao grafo acíclico direcionado de transformações, o qual é executado apenas quando uma ação sobre o RDD em questão for solicitada. Essa característica é conhecida como lazy-evaluation.\n","\n","Como resultado, operações podem ser executadas em conjunto, possibilitando otimizações. Por exemplo, os métodos `map()` e `reduceByKey()` são transformações, e eles somente são executados quando ocorrer uma ação, como quando o método collect() for chamado. Portanto, `map()`, `reduceByKey()` e `collect()` podem ser executados em conjunto.\n","\n","No exemplo do contador de palavras, comumente utiliza-se o que é conhecido como side-map reducer, indicando que internamente o Spark usa uma estrutura de HashMap e faz o método de `reduceByKey()` localmente antes de enviar o resultado para a etapa de reducing propriamente dita. Isso é ilustrado por meio da seta azul na figura a seguir.\n","\n","Por outro lado, o framework Hadoop não provê operações com a característica de *lazy-evaluation*. Portanto, operações não são executadas em conjunto. Refaça a figura a seguir de forma que o exemplo do contador de palavras não considere essa característica, ou seja, que o contador de palavras não considere a característica de *lazy-evaluation*.\n","\n","**Dica para o exercício:** Ao invés de refazer a figura, é possível discutir as alterações que precisam ser feitas."],"metadata":{"id":"kuY5OZ3XQYMJ"}},{"cell_type":"markdown","source":["<br>\n","\n","<p align=\"center\"><img src=\"https://raw.githubusercontent.com/kenjitakatuzi/MBA-ICMC-2022/main/images/FIGURA1.jpeg\" width=\"600\" height=\"300\"></p>\n","<!-- <p align=\"center\">Figura 2 - Exemplo de contador de palavras.</p> -->\n","\n","<br>"],"metadata":{"id":"WwCrgjuAQ9Mz"}},{"cell_type":"markdown","source":["\n","\n","**Resposta do exercício:**"],"metadata":{"id":"D7EJc8MJM20d"}}]}